{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# IndoXTC - Fine-tune Full Toxic [XLM-R] Comparable\n",
    "Exploring Indonesian hate speech/abusive & sentiment text classification using multilingual language model.\n",
    "\n",
    "This kernel is a part of my undergraduate final year project.\n",
    "Checkout the full github repository:\n",
    "https://github.com/ilhamfp/indonesian-text-classification-multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Experiment configurations #\n",
    "#############################\n",
    "\n",
    "INDO_DATA_NAME = 'toxic'\n",
    "DATA_PATH_INDO = '../input/simpler-preprocess-indonesian-hate-abusive-text'\n",
    "\n",
    "FOREIGN_DATA_NAME = 'toxic'\n",
    "DATA_PATH_FOREIGN = '../input/jigsaw-multilingual-toxic-comment-classification'\n",
    "\n",
    "MODEL_NAME = 'XLM_R'\n",
    "\n",
    "EXPERIMENT_TYPE = 'A' # A / B / C\n",
    "TOTAL_DATA = 11852 # 500 / 1000 / 2500 / 5000 / 7500 / 11852\n",
    "FOREIGN_LANG_DATA_MULT = 0.5 # 0.5 / 1 / 1.5 / 2 / 3\n",
    "RANDOM_SEED = 1\n",
    "VALIDATION_DATA = 0.1\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 5e-6\n",
    "USE_TPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "######\n",
    "## model_full\n",
    "######\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors, SentencePieceBPETokenizer\n",
    "from tensorflow.keras import backend as K\n",
    "    \n",
    "def set_seed(seed=1):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def build_model(transformer, learning_rate=1e-5, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    cls_token = Dropout(0.2)(cls_token)\n",
    "    out = Dense(2, activation='sigmoid')(cls_token)\n",
    "    \n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def callback():\n",
    "    cb = []\n",
    "\n",
    "    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss',  \n",
    "                                       factor=0.5, patience=2, \n",
    "                                       verbose=1, mode='min', \n",
    "                                       epsilon=0.0001, min_lr=0,\n",
    "                                       restore_best_weights=True)\n",
    "    cb.append(reduceLROnPlat)\n",
    "    \n",
    "    log = CSVLogger('log.csv')\n",
    "    cb.append(log)\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', patience=4, verbose=0,\n",
    "                       mode='min', restore_best_weights=True)\n",
    "    \n",
    "    cb.append(es)\n",
    "    \n",
    "    return cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "######\n",
    "## load_data\n",
    "######\n",
    "\n",
    "# This source code is part of a final year undergraduate project\n",
    "# on exploring Indonesian hate speech/abusive & sentiment text \n",
    "# classification using a multilingual language model\n",
    "# \n",
    "# Checkout the full github repository: \n",
    "# https://github.com/ilhamfp/indonesian-text-classification-multilingual\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "RANDOM_SEED=1\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_nonaplhanumeric(text):\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "    return text\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "    text = re.sub('\\n',' ',text) # Remove every '\\n'\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = lowercase(text)\n",
    "    text = remove_nonaplhanumeric(text)\n",
    "    text = remove_unnecessary_char(text)\n",
    "    return text\n",
    "\n",
    "def load_dataset_indonesian(data_name='prosa', data_path=None, data_path_test=None):\n",
    "    if data_name == 'prosa':\n",
    "        train = pd.read_csv('../input/dataset-prosa/data_train_full.tsv', sep='\\t', header=None)\n",
    "        train = train.rename(columns={0: \"text\", 1: \"label\"})\n",
    "        train = train[train['label'] != 'neutral']\n",
    "        train['label'] = train['label'].apply(lambda x: 1 if x=='positive' else 0)\n",
    "        train['text'] = train['text'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "        test = pd.read_csv('../input/dataset-prosa/data_testing_full.tsv', sep='\\t', header=None)\n",
    "        test = test.rename(columns={0: \"text\", 1: \"label\"})\n",
    "        test = test[test['label'] != 'neutral']\n",
    "        test['label'] = test['label'].apply(lambda x: 1 if x=='positive' else 0)\n",
    "        test['text'] = test['text'].apply(lambda x: preprocess_text(x))\n",
    "            \n",
    "    elif data_name == 'trip_advisor':\n",
    "        if data_path == None:\n",
    "            train = pd.read_csv('../input/dataset-tripadvisor/train_set.csv')\n",
    "#             train = pd.read_csv('../input/remove-duplicate-tripadvisor/train_set.csv')\n",
    "        else:\n",
    "            train = pd.read_csv(data_path)\n",
    "            \n",
    "        train = train.rename(columns={\"content\": \"text\", \"polarity\": \"label\"})\n",
    "        train['label'] = train['label'].apply(lambda x: 1 if x==\"positive\" else 0)\n",
    "        train['text'] = train['text'].apply(lambda x: preprocess_text(x))\n",
    "        \n",
    "        if data_path_test == None:\n",
    "            test = pd.read_csv('../input/dataset-tripadvisor/test_set.csv')\n",
    "#             test = pd.read_csv('../input/remove-duplicate-tripadvisor/test_set.csv')\n",
    "        else:\n",
    "            test = pd.read_csv(data_path_test)\n",
    "            \n",
    "        test = test.rename(columns={\"content\": \"text\", \"polarity\": \"label\"})\n",
    "        test['label'] = test['label'].apply(lambda x: 1 if x==\"positive\" else 0)\n",
    "        test['text'] = test['text'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "    elif data_name == 'toxic':\n",
    "        if data_path == None:\n",
    "            data = pd.read_csv('../input/simpler-preprocess-indonesian-hate-abusive-text/preprocessed_indonesian_toxic_tweet.csv')\n",
    "        else:\n",
    "            data = pd.read_csv(data_path)\n",
    "            \n",
    "        temp = pd.DataFrame({\n",
    "                   'HS': data['HS'].values,\n",
    "                   'Abusive': data['Abusive'].values})\n",
    "\n",
    "        data['label'] = temp.apply(lambda r: tuple(r), axis=1).apply(np.array)\n",
    "            \n",
    "        data = data[['Tweet', 'label']]\n",
    "        data = data.rename(columns={'Tweet': 'text'})\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data.text.values, \n",
    "                                                            data.label.values, \n",
    "                                                            test_size=0.1,\n",
    "                                                            random_state=RANDOM_SEED)\n",
    "        train = pd.DataFrame({'text': X_train,\n",
    "                              'label': y_train})\n",
    "\n",
    "        test = pd.DataFrame({'text': X_test,\n",
    "                             'label': y_test})\n",
    "        \n",
    "    print(\"~~~Train Data~~~\")\n",
    "    print('Shape: ', train.shape)\n",
    "    print(train[0:2])\n",
    "    print(\"\\nLabel:\")\n",
    "#     print(train.label.value_counts())\n",
    "    \n",
    "    print(\"\\n~~~Test Data~~~\")\n",
    "    print('Shape: ', test.shape)\n",
    "    print(test[0:4])\n",
    "    print(\"\\nLabel:\")\n",
    "#     print(test.label.value_counts())\n",
    "    return train, test\n",
    "    \n",
    "def load_dataset_foreign(data_name='yelp'):\n",
    "    train = None\n",
    "    if data_name == 'yelp':\n",
    "        train = pd.read_csv('../input/yelp-review-dataset/yelp_review_polarity_csv/train.csv', header=None)\n",
    "        train = train.rename(columns={0: \"label\", 1: \"text\"})\n",
    "        train['label'] = train['label'].apply(lambda x: 1 if x==2 else 0)\n",
    "        train['text'] = train['text'].apply(lambda x: preprocess_text(x))\n",
    "    \n",
    "    elif data_name == 'toxic':\n",
    "        data = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv')\n",
    "        data['toxic'] = data['toxic'].apply(lambda x: 1 if x>=0.5 else 0)\n",
    "\n",
    "        data = data[['comment_text', 'toxic']]\n",
    "        data = data.rename(columns={'comment_text': 'text',\n",
    "                                    'toxic': 'label'})\n",
    "\n",
    "        data_pos = data[data['label'] == 1]\n",
    "        data_neg = data[data['label'] == 0]\n",
    "        train = pd.concat([data_pos[0:152111], \n",
    "                           data_neg[0:152111]]).reset_index(drop=True)\n",
    "        \n",
    "        train['text'] = train['text'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "     \n",
    "    print(\"~~~Data~~~\")\n",
    "    print('Shape: ', train.shape)\n",
    "    print(train[0:2])\n",
    "    print(\"\\nLabel:\")\n",
    "    print(train.label.value_counts())\n",
    "    return train\n",
    "\n",
    "def split_train_test(train_x, train_y, total_data=50, valid_size=0.2):\n",
    "    train_x_split, valid_x_split, train_y_split, valid_y_split = train_test_split(train_x, \n",
    "                                                                                  train_y, \n",
    "                                                                                  test_size=valid_size,\n",
    "                                                                                  random_state=RANDOM_SEED)\n",
    "    \n",
    "        \n",
    "    total_data_valid = int(np.floor(valid_size * total_data))\n",
    "    total_data_train = total_data-total_data_valid\n",
    "\n",
    "    train_x_split = train_x_split[:total_data_train]\n",
    "    train_y_split = train_y_split[:total_data_train]\n",
    "    valid_x_split = valid_x_split[:total_data_valid]\n",
    "    valid_y_split = valid_y_split[:total_data_valid]\n",
    "    \n",
    "    return train_x_split, train_y_split, valid_x_split, valid_y_split\n",
    "    \n",
    "def load_features(data_path, total_data=50, valid_size=0.2):\n",
    "    train_x = np.array([x for x in np.load('{}/train_text.npy'.format(data_path), allow_pickle=True)])\n",
    "    train_y = pd.read_csv('{}/train_label.csv'.format(data_path)).label.values\n",
    "    \n",
    "    train_x_split, train_y_split, valid_x_split, valid_y_split = split_train_test(train_x,\n",
    "                                                                                  train_y,\n",
    "                                                                                  total_data=total_data,\n",
    "                                                                                  valid_size=valid_size)\n",
    "    return train_x_split, train_y_split, valid_x_split, valid_y_split\n",
    "    \n",
    "\n",
    "def load_experiment_features(data_path_indo,\n",
    "                             data_path_foreign,\n",
    "                             tipe='A', \n",
    "                             total_data=50, \n",
    "                             foreign_mult=1, \n",
    "                             valid_size=0.2,\n",
    "                             ):\n",
    "    ##########################\n",
    "    # Load Preprocessed Data #\n",
    "    ##########################\n",
    "    if tipe == 'A':\n",
    "        train_x, train_y, valid_x, valid_y = load_features(data_path_indo,\n",
    "                                                           total_data=total_data, \n",
    "                                                           valid_size=valid_size)\n",
    "        \n",
    "    elif tipe == 'B':\n",
    "        train_x, train_y, _, _ = load_features(data_path_foreign,\n",
    "                                               total_data=total_data, \n",
    "                                               valid_size=valid_size)\n",
    "        \n",
    "        _, _, valid_x, valid_y = load_features(data_path_indo,\n",
    "                                               total_data=total_data, \n",
    "                                               valid_size=valid_size)\n",
    "        \n",
    "    elif tipe == 'C':\n",
    "        train_x_indo, train_y_indo, valid_x_indo, valid_y_indo = load_features(data_path_indo,\n",
    "                                                                                total_data=total_data, \n",
    "                                                                                valid_size=valid_size)\n",
    "\n",
    "        train_x_foreign, train_y_foreign, valid_x_foreign, valid_y_foreign = load_features(data_path_foreign,\n",
    "                                                                                           total_data=int(total_data*foreign_mult), \n",
    "                                                                                           valid_size=valid_size)\n",
    "\n",
    "        train_x = np.concatenate([\n",
    "                    train_x_indo,\n",
    "                    train_x_foreign,\n",
    "                    ])\n",
    "\n",
    "        train_y = np.concatenate([\n",
    "                    train_y_indo,\n",
    "                    train_y_foreign,\n",
    "                ])\n",
    "\n",
    "        valid_x = valid_x_indo\n",
    "\n",
    "        valid_y = valid_y_indo\n",
    "        \n",
    "\n",
    "    test_x = np.array([x for x in np.load('{}/test_text.npy'.format(data_path_indo), allow_pickle=True)])\n",
    "    test_y = pd.read_csv('{}/test_label.csv'.format(data_path_indo)).label.values\n",
    "\n",
    "    #########################\n",
    "    # Convert to dataloader #\n",
    "    #########################\n",
    "    batch_size = 32\n",
    "\n",
    "    train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "    valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "    test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "def load_train_dataset(data_name, total_data=50, valid_size=0.2, is_foreign=False):\n",
    "    print('sampai')\n",
    "\n",
    "    train = None\n",
    "    if is_foreign:\n",
    "        train = load_dataset_foreign(data_name)\n",
    "    else:\n",
    "        train, test = load_dataset_indonesian(data_name)\n",
    "\n",
    "    print('sampai2')\n",
    "\n",
    "    train_x_split, train_y_split, valid_x_split, valid_y_split = split_train_test(train.text.values,\n",
    "                                                                                  train.label.values,\n",
    "                                                                                  total_data=total_data,\n",
    "                                                                                  valid_size=valid_size)\n",
    "    print('sampai3')\n",
    "\n",
    "    train_x_split = np.array([x for x in train_x_split])\n",
    "    valid_x_split = np.array([x for x in valid_x_split])\n",
    "    return train_x_split, train_y_split, valid_x_split, valid_y_split\n",
    "\n",
    "def load_experiment_dataset(data_name_indo,\n",
    "                            data_name_foreign,\n",
    "                            tipe='A', \n",
    "                            total_data=50, \n",
    "                            foreign_mult=1, \n",
    "                            valid_size=0.2):\n",
    "    \n",
    "    #################\n",
    "    # Load Raw Data #\n",
    "    #################\n",
    "    if tipe == 'A':\n",
    "        train_x, train_y, valid_x, valid_y = load_train_dataset(data_name_indo,\n",
    "                                                                total_data=total_data, \n",
    "                                                                valid_size=valid_size,\n",
    "                                                                is_foreign=False)\n",
    "        \n",
    "    elif tipe == 'B':\n",
    "        train_x, train_y, _, _ = load_train_dataset(data_name_foreign,\n",
    "                                                    total_data=total_data, \n",
    "                                                    valid_size=valid_size,\n",
    "                                                    is_foreign=True)\n",
    "        \n",
    "        _, _, valid_x, valid_y = load_train_dataset(data_name_indo,\n",
    "                                                    total_data=total_data, \n",
    "                                                    valid_size=valid_size,\n",
    "                                                    is_foreign=False)\n",
    "        \n",
    "    elif tipe == 'C':\n",
    "        train_x_indo, train_y_indo, valid_x_indo, valid_y_indo = load_train_dataset(data_name_indo,\n",
    "                                                                                    total_data=total_data, \n",
    "                                                                                    valid_size=valid_size,\n",
    "                                                                                    is_foreign=False)\n",
    "\n",
    "        train_x_foreign, train_y_foreign, valid_x_foreign, valid_y_foreign = load_train_dataset(data_name_foreign,\n",
    "                                                                                                total_data=int(total_data*foreign_mult), \n",
    "                                                                                                valid_size=valid_size,\n",
    "                                                                                                is_foreign=True)\n",
    "\n",
    "        train_x = np.concatenate([\n",
    "                    train_x_indo,\n",
    "                    train_x_foreign,\n",
    "                    ])\n",
    "\n",
    "        train_y = np.concatenate([\n",
    "                    train_y_indo,\n",
    "                    train_y_foreign,\n",
    "                ])\n",
    "\n",
    "        valid_x = valid_x_indo\n",
    "\n",
    "        valid_y = valid_y_indo\n",
    "        \n",
    "    \n",
    "\n",
    "    _, test = load_dataset_indonesian(data_name=data_name_indo)\n",
    "    test_x = test.text.values\n",
    "    test_x = np.array([x for x in test_x])\n",
    "    test_y = test.label.values\n",
    "    \n",
    "    indices = np.arange(len(train_x))\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    np.random.shuffle(indices)\n",
    "    train_x = train_x[indices]\n",
    "    train_y = train_y[indices]\n",
    "\n",
    "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "set_seed(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## TPU Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "if USE_TPU:\n",
    "    # Detect hardware, return appropriate distribution strategy\n",
    "    try:\n",
    "        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "        # set: this is always the case on Kaggle.\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    else:\n",
    "        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "\n",
    "    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "    BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n",
    "    \n",
    "else:\n",
    "    BATCH_SIZE = 8 * 8\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "MAX_LEN = 512\n",
    "\n",
    "if MODEL_NAME == 'XLM_R':\n",
    "    MODEL = 'jplu/tf-xlm-roberta-large'\n",
    "elif MODEL_NAME == 'mBERT':\n",
    "    MODEL = 'bert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampai\n",
      "~~~Train Data~~~\n",
      "Shape:  (11852, 2)\n",
      "                                                text   label\n",
      "0   aku sebenarnya antek antek nya juga wkwk teta...  [0, 0]\n",
      "1  jauh sebelum menjadi wakil presiden susilo bam...  [0, 0]\n",
      "\n",
      "Label:\n",
      "\n",
      "~~~Test Data~~~\n",
      "Shape:  (1317, 2)\n",
      "                                                text   label\n",
      "0   cebong produsen hoaks tukang bohong dan ingka...  [1, 1]\n",
      "1     jangan cepat cepat ketawanya kayak kunti saja   [0, 0]\n",
      "2                                          berbicara  [0, 0]\n",
      "3  kelompok mana yang berhasil mengkapitalisasi g...  [1, 0]\n",
      "\n",
      "Label:\n",
      "sampai2\n",
      "sampai3\n",
      "~~~Train Data~~~\n",
      "Shape:  (11852, 2)\n",
      "                                                text   label\n",
      "0   aku sebenarnya antek antek nya juga wkwk teta...  [0, 0]\n",
      "1  jauh sebelum menjadi wakil presiden susilo bam...  [0, 0]\n",
      "\n",
      "Label:\n",
      "\n",
      "~~~Test Data~~~\n",
      "Shape:  (1317, 2)\n",
      "                                                text   label\n",
      "0   cebong produsen hoaks tukang bohong dan ingka...  [1, 1]\n",
      "1     jangan cepat cepat ketawanya kayak kunti saja   [0, 0]\n",
      "2                                          berbicara  [0, 0]\n",
      "3  kelompok mana yang berhasil mengkapitalisasi g...  [1, 0]\n",
      "\n",
      "Label:\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_valid, y_valid), (x_test, y_test) = load_experiment_dataset(INDO_DATA_NAME,\n",
    "                                                                                   FOREIGN_DATA_NAME,\n",
    "                                                                                   tipe=EXPERIMENT_TYPE, \n",
    "                                                                                   total_data=TOTAL_DATA, \n",
    "                                                                                   foreign_mult=FOREIGN_LANG_DATA_MULT, \n",
    "                                                                                   valid_size=VALIDATION_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([np.array([x[0], x[1]]) for x in y_train])\n",
    "y_valid = np.array([np.array([x[0], x[1]]) for x in y_valid])\n",
    "y_test = np.array([np.array([x[0], x[1]]) for x in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10666, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d44d6f5e76840ab8d73b2611c73b392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=513.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6802566682c4d8c9d4bf9b4b145392b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.41 s, sys: 18.6 ms, total: 4.42 s\n",
      "Wall time: 4.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "x_train = regular_encode(x_train, tokenizer, maxlen=MAX_LEN)\n",
    "x_valid = regular_encode(x_valid, tokenizer, maxlen=MAX_LEN)\n",
    "x_test = regular_encode(x_test, tokenizer, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Build Datasets Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(len(x_train),\n",
    "             seed=RANDOM_SEED)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_valid, y_valid))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653238f95b914658812752ee0d7db16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3271420488.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "tf_roberta_model (TFRobertaM ((None, 512, 1024), (None 559890432 \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice (T [(None, 1024)]            0         \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 559,892,482\n",
      "Trainable params: 559,892,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 2min 9s, sys: 42.7 s, total: 2min 51s\n",
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if USE_TPU:\n",
    "    with strategy.scope():\n",
    "        transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
    "        model = build_model(transformer_layer, max_len=MAX_LEN, learning_rate=LEARNING_RATE)\n",
    "        \n",
    "else:\n",
    "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
    "    model = build_model(transformer_layer, max_len=MAX_LEN, learning_rate=LEARNING_RATE)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n"
     ]
    }
   ],
   "source": [
    "n_steps = x_train.shape[0] // BATCH_SIZE\n",
    "print(n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 166 steps, validate for 19 steps\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 331s 2s/step - loss: 0.7462 - accuracy: 0.5242 - val_loss: 0.6717 - val_accuracy: 0.5726\n",
      "Epoch 2/25\n",
      "166/166 [==============================] - 122s 737ms/step - loss: 0.7003 - accuracy: 0.5691 - val_loss: 0.6484 - val_accuracy: 0.5996\n",
      "Epoch 3/25\n",
      "166/166 [==============================] - 122s 736ms/step - loss: 0.6793 - accuracy: 0.5842 - val_loss: 0.6060 - val_accuracy: 0.6485\n",
      "Epoch 4/25\n",
      "166/166 [==============================] - 123s 740ms/step - loss: 0.6352 - accuracy: 0.6342 - val_loss: 0.5769 - val_accuracy: 0.6873\n",
      "Epoch 5/25\n",
      "166/166 [==============================] - 123s 741ms/step - loss: 0.5638 - accuracy: 0.7005 - val_loss: 0.4918 - val_accuracy: 0.7532\n",
      "Epoch 6/25\n",
      "166/166 [==============================] - 122s 734ms/step - loss: 0.4990 - accuracy: 0.7577 - val_loss: 0.4423 - val_accuracy: 0.7916\n",
      "Epoch 7/25\n",
      "166/166 [==============================] - 122s 737ms/step - loss: 0.4407 - accuracy: 0.7946 - val_loss: 0.3644 - val_accuracy: 0.8439\n",
      "Epoch 8/25\n",
      "166/166 [==============================] - 116s 698ms/step - loss: 0.3975 - accuracy: 0.8205 - val_loss: 0.4456 - val_accuracy: 0.8000\n",
      "Epoch 9/25\n",
      "165/166 [============================>.] - ETA: 0s - loss: 0.3539 - accuracy: 0.8442\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "166/166 [==============================] - 116s 697ms/step - loss: 0.3545 - accuracy: 0.8437 - val_loss: 0.3979 - val_accuracy: 0.8325\n",
      "Epoch 10/25\n",
      "166/166 [==============================] - 122s 737ms/step - loss: 0.3058 - accuracy: 0.8681 - val_loss: 0.3058 - val_accuracy: 0.8747\n",
      "Epoch 11/25\n",
      "166/166 [==============================] - 116s 699ms/step - loss: 0.2857 - accuracy: 0.8795 - val_loss: 0.3255 - val_accuracy: 0.8878\n",
      "Epoch 12/25\n",
      "165/166 [============================>.] - ETA: 0s - loss: 0.2747 - accuracy: 0.8830\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "166/166 [==============================] - 116s 697ms/step - loss: 0.2749 - accuracy: 0.8828 - val_loss: 0.3346 - val_accuracy: 0.8797\n",
      "Epoch 13/25\n",
      "166/166 [==============================] - 116s 699ms/step - loss: 0.2483 - accuracy: 0.8958 - val_loss: 0.3254 - val_accuracy: 0.8844\n",
      "Epoch 14/25\n",
      "166/166 [==============================] - 122s 735ms/step - loss: 0.2397 - accuracy: 0.8999 - val_loss: 0.3046 - val_accuracy: 0.8928\n",
      "Epoch 15/25\n",
      "166/166 [==============================] - 116s 698ms/step - loss: 0.2450 - accuracy: 0.8955 - val_loss: 0.3198 - val_accuracy: 0.8848\n",
      "Epoch 16/25\n",
      "165/166 [============================>.] - ETA: 0s - loss: 0.2313 - accuracy: 0.9018\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "166/166 [==============================] - 116s 698ms/step - loss: 0.2308 - accuracy: 0.9022 - val_loss: 0.3100 - val_accuracy: 0.8924\n",
      "Epoch 17/25\n",
      "166/166 [==============================] - 116s 698ms/step - loss: 0.2229 - accuracy: 0.9078 - val_loss: 0.3365 - val_accuracy: 0.8810\n",
      "Epoch 18/25\n",
      "165/166 [============================>.] - ETA: 0s - loss: 0.2175 - accuracy: 0.9076\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "166/166 [==============================] - 145s 875ms/step - loss: 0.2176 - accuracy: 0.9074 - val_loss: 0.3395 - val_accuracy: 0.8861\n"
     ]
    }
   ],
   "source": [
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=n_steps,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks = callback(), \n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 47s 2s/step\n"
     ]
    }
   ],
   "source": [
    "test_prediction = model.predict(test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_HS = pd.DataFrame()\n",
    "result_HS['y_pred'] = test_prediction[:, 0]\n",
    "result_HS['y_true'] = y_test[:, 0]\n",
    "result_HS.to_csv('result_HS_{}_{}_{}_{}_{}_{}_full.csv'.format(INDO_DATA_NAME,\n",
    "                                                    FOREIGN_DATA_NAME,\n",
    "                                                    MODEL_NAME,\n",
    "                                                    EXPERIMENT_TYPE,\n",
    "                                                    TOTAL_DATA,\n",
    "                                                    FOREIGN_LANG_DATA_MULT),\n",
    "              index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.985429</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.056491</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.075079</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.734175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.475477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_pred  y_true\n",
       "0  0.985429       1\n",
       "1  0.056491       0\n",
       "2  0.075079       0\n",
       "3  0.734175       1\n",
       "4  0.475477       0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_HS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_Abusive = pd.DataFrame()\n",
    "result_Abusive['y_pred'] = test_prediction[:, 1]\n",
    "result_Abusive['y_true'] = y_test[:, 1]\n",
    "result_Abusive.to_csv('result_Abusive_{}_{}_{}_{}_{}_{}_full.csv'.format(INDO_DATA_NAME,\n",
    "                                                    FOREIGN_DATA_NAME,\n",
    "                                                    MODEL_NAME,\n",
    "                                                    EXPERIMENT_TYPE,\n",
    "                                                    TOTAL_DATA,\n",
    "                                                    FOREIGN_LANG_DATA_MULT),\n",
    "              index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.996780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047971</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.310369</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.031304</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.785427</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_pred  y_true\n",
       "0  0.996780       1\n",
       "1  0.047971       0\n",
       "2  0.310369       0\n",
       "3  0.031304       0\n",
       "4  0.785427       1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_Abusive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "07126c5ee4e44a568444b9027a8228e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1b56c05c74ca4a45b3249819f7a22410",
       "placeholder": "​",
       "style": "IPY_MODEL_40b277ff7fe6434ea86a402ba71abb7d",
       "value": " 3.27G/3.27G [01:51&lt;00:00, 29.2MB/s]"
      }
     },
     "07417a4f6055497c906fde91c2da8acd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f81b2dfaa3764318aa06d69d22939f4a",
       "max": 513.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3bf3f324d1034020be4ed900b842d853",
       "value": 513.0
      }
     },
     "09c2c2164be74177b21095ea2e5a29ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "1b56c05c74ca4a45b3249819f7a22410": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1dd1bb10fb7447fe8855317ecec7949a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_24e5147f987b42cb985807b8bfb0c6c4",
       "placeholder": "​",
       "style": "IPY_MODEL_90ceab05f88940f58f0ceb6db81e9c81",
       "value": " 513/513 [00:00&lt;00:00, 3.62kB/s]"
      }
     },
     "24e5147f987b42cb985807b8bfb0c6c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3bf3f324d1034020be4ed900b842d853": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "3d44d6f5e76840ab8d73b2611c73b392": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_07417a4f6055497c906fde91c2da8acd",
        "IPY_MODEL_1dd1bb10fb7447fe8855317ecec7949a"
       ],
       "layout": "IPY_MODEL_ab2799ddfb1b4caba9d3fac21ebd1079"
      }
     },
     "40b277ff7fe6434ea86a402ba71abb7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "50baeaffe4704b2ebb1602e812b0252f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e88982c9699e489f997814460f85feaf",
       "max": 5069051.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_09c2c2164be74177b21095ea2e5a29ef",
       "value": 5069051.0
      }
     },
     "596fa827ac9d410398df7010a2e23e84": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "653238f95b914658812752ee0d7db16d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_77b815c4725540ad83e96ee28414bcbf",
        "IPY_MODEL_07126c5ee4e44a568444b9027a8228e5"
       ],
       "layout": "IPY_MODEL_dc3086bcb3c247ce9e128e7c61b49c1f"
      }
     },
     "77b815c4725540ad83e96ee28414bcbf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_596fa827ac9d410398df7010a2e23e84",
       "max": 3271420488.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_96f408944ba347eca1ade4fd715aaf4f",
       "value": 3271420488.0
      }
     },
     "90ceab05f88940f58f0ceb6db81e9c81": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "96f408944ba347eca1ade4fd715aaf4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "9e6c03ca05774a638c3c11d15c8ad85b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ab2799ddfb1b4caba9d3fac21ebd1079": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b54f6f6983514276a1934ea169469e4a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c4e515b3bf9e4e0d9cfd05f4daf5ba50": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ce8283f95b1e48ea9cfe4d34e1023fca",
       "placeholder": "​",
       "style": "IPY_MODEL_9e6c03ca05774a638c3c11d15c8ad85b",
       "value": " 5.07M/5.07M [00:04&lt;00:00, 1.04MB/s]"
      }
     },
     "ce8283f95b1e48ea9cfe4d34e1023fca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dc3086bcb3c247ce9e128e7c61b49c1f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e88982c9699e489f997814460f85feaf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f6802566682c4d8c9d4bf9b4b145392b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_50baeaffe4704b2ebb1602e812b0252f",
        "IPY_MODEL_c4e515b3bf9e4e0d9cfd05f4daf5ba50"
       ],
       "layout": "IPY_MODEL_b54f6f6983514276a1934ea169469e4a"
      }
     },
     "f81b2dfaa3764318aa06d69d22939f4a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
