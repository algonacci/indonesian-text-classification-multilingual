@article{Liu2012, title={Sentiment Analysis and Opinion Mining}, volume={5}, ISSN={1947-4040}, DOI={10.2200/S00416ED1V01Y201204HLT016}, number={1}, journal={Synthesis Lectures on Human Language Technologies}, author={Liu, Bing}, year={2012}, month={May}, pages={1–167} }

@article{LampleConneau2019, title={Cross-lingual Language Model Pretraining}, url={http://arxiv.org/abs/1901.07291}, abstractNote={Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On `XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT’16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT’16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.}, note={arXiv: 1901.07291}, journal={arXiv:1901.07291 [cs]}, author={Lample, Guillaume and Conneau, Alexis}, year={2019}, month={Jan} }

@article{CrisdayantiPurwarianti2019,
    title={Improving Bi-LSTM Performance for Indonesian Sentiment Analysis Using Paragraph Vector},
    author={Crisdayanti, I. A. P. A. and Purwarianti, Ayu},
    year={2019},
    publisher={In International Conference on Advanced Informatics: Concepts, Theory and Applications 2019}
}

@inproceedings{VADER,
  author = {Hutto, Clayton J. and Gilbert, Eric},
  biburl = {https://www.bibsonomy.org/bibtex/253a5741ab827539d4bc4bf836fa6dae7/dblp},
  booktitle = {ICWSM},
  editor = {Adar, Eytan and Resnick, Paul and Choudhury, Munmun De and Hogan, Bernie and Oh, Alice H.},
  ee = {http://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/view/8109},
  interhash = {e73f21d7ff21014b85f5c7b48f335a11},
  intrahash = {53a5741ab827539d4bc4bf836fa6dae7},
  isbn = {978-1-57735-659-2},
  keywords = {dblp},
  publisher = {The AAAI Press},
  timestamp = {2016-05-10T11:39:40.000+0200},
  title = {VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text.},
  url = {http://dblp.uni-trier.de/db/conf/icwsm/icwsm2014.html#HuttoG14},
  year = {2014}
}

@article{GLUE2019, title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}, url={http://arxiv.org/abs/1804.07461}, abstractNote={For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.}, note={arXiv: 1804.07461}, journal={arXiv:1804.07461 [cs]}, author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.}, year={2019}, month={Feb} }

@inproceedings{ruder2019transfer,
  title={Transfer Learning in Natural Language Processing},
  author={Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials},
  pages={15--18},
  year={2019}
}

@article{ruder2019survey, title={A Survey Of Cross-lingual Word Embedding Models}, volume={65}, ISSN={1076-9757}, DOI={10.1613/jair.1.11640}, abstractNote={Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.}, note={arXiv: 1706.04902}, journal={Journal of Artificial Intelligence Research}, author={Ruder, Sebastian and Vulić, Ivan and Søgaard, Anders}, year={2019}, month={Aug}, pages={569–631} }

@article{MikolovWord2vec, title={Distributed Representations of Words and Phrases and their Compositionality}, url={http://arxiv.org/abs/1310.4546}, abstractNote={The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.}, note={arXiv: 1310.4546}, journal={arXiv:1310.4546 [cs, stat]}, author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey}, year={2013}, month={Oct} }

@article{MikolovExploiting, title={Exploiting Similarities among Languages for Machine Translation}, url={http://arxiv.org/abs/1309.4168}, abstractNote={Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.}, note={arXiv: 1309.4168}, journal={arXiv:1309.4168 [cs]}, author={Mikolov, Tomas and Le, Quoc V. and Sutskever, Ilya}, year={2013}, month={Sep} }

@article{MikolovEstimation, title={Efficient Estimation of Word Representations in Vector Space}, url={http://arxiv.org/abs/1301.3781}, abstractNote={We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.}, note={arXiv: 1301.3781}, journal={arXiv:1301.3781 [cs]}, author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey}, year={2013}, month={Sep} }

@inproceedings{MikolovLinguistic2013, place={Atlanta, Georgia}, title={Linguistic Regularities in Continuous Space Word Representations}, url={https://www.aclweb.org/anthology/N13-1090}, booktitle={Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, publisher={Association for Computational Linguistics}, author={Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey}, year={2013}, month={Jun}, pages={746–751} }

@inproceedings{Luong_Pham_Manning_2015, place={Denver, Colorado}, title={Bilingual Word Representations with Monolingual Quality in Mind}, url={https://www.aclweb.org/anthology/W15-1521}, DOI={10.3115/v1/W15-1521}, booktitle={Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing}, publisher={Association for Computational Linguistics}, author={Luong, Thang and Pham, Hieu and Manning, Christopher D.}, year={2015}, month={Jun}, pages={151–159} }

@book{Zhu_Davidson_2007, place={Hershey}, title={Knowledge discovery and data mining: challenges and realities}, ISBN={9781599042527}, abstractNote={“This book provides a focal point for research and real-world data mining practitioners that advance knowledge discovery from low-quality data; it presents in-depth experiences and methodologies, providing theoretical and empirical guidance to users who have suffered from underlying low-quality data. Contributions also focus on interdisciplinary collaborations among data quality, data processing, data mining, data privacy, and data sharing”--Provided by publisher}, publisher={Information Science Reference}, 
author={Zhu, Xingquan},
year={2007} }


@article{Lai_Oguz_Yang_Stoyanov_2019, title={Bridging the domain gap in cross-lingual document classification}, url={http://arxiv.org/abs/1909.07009}, abstractNote={The scarcity of labeled training data often prohibits the internationalization of NLP models to multiple languages. Recent developments in cross-lingual understanding (XLU) has made progress in this area, trying to bridge the language barrier using language universal representations. However, even if the language problem was resolved, models trained in one language would not transfer to another language perfectly due to the natural domain drift across languages and cultures. We consider the setting of semi-supervised cross-lingual understanding, where labeled data is available in a source language (English), but only unlabeled data is available in the target language. We combine state-of-the-art cross-lingual methods with recently proposed methods for weakly supervised learning such as unsupervised pre-training and unsupervised data augmentation to simultaneously close both the language gap and the domain gap in XLU. We show that addressing the domain gap is crucial. We improve over strong baselines and achieve a new state-of-the-art for cross-lingual document classification.}, note={arXiv: 1909.07009}, journal={arXiv:1909.07009 [cs]}, author={Lai, Guokun and Oguz, Barlas and Yang, Yiming and Stoyanov, Veselin}, year={2019}, month={Sep} }


@article{Artetxe_Schwenk_2019, title={Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond}, url={http://arxiv.org/abs/1812.10464}, abstractNote={We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different families and written in 28 different scripts. Our system uses a single BiLSTM encoder with a shared BPE vocabulary for all languages, which is coupled with an auxiliary decoder and trained on publicly available parallel corpora. This enables us to learn a classifier on top of the resulting embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification. Our experiments in cross-lingual natural language inference (XNLI dataset), cross-lingual document classification (MLDoc dataset) and parallel corpus mining (BUCC dataset) show the effectiveness of our approach. We also introduce a new test set of aligned sentences in 112 languages, and show that our sentence embeddings obtain strong results in multilingual similarity search even for low-resource languages. Our implementation, the pre-trained encoder and the multilingual test set are available at https://github.com/facebookresearch/LASER}, note={arXiv: 1812.10464}, journal={arXiv:1812.10464 [cs]}, author={Artetxe, Mikel and Schwenk, Holger}, year={2019}, month={Sep} }


@inproceedings{Conneau_Rinott_Lample_Williams_Bowman_Schwenk_Stoyanov_2018, place={Brussels, Belgium}, title={XNLI: Evaluating Cross-lingual Sentence Representations}, url={https://www.aclweb.org/anthology/D18-1269}, DOI={10.18653/v1/D18-1269}, abstractNote={State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines.}, booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, publisher={Association for Computational Linguistics}, author={Conneau, Alexis and Rinott, Ruty and Lample, Guillaume and Williams, Adina and Bowman, Samuel and Schwenk, Holger and Stoyanov, Veselin}, year={2018}, month={Oct}, pages={2475–2485} }


@article{Wang_Xie_Xu_Yang_Neubig_Carbonell_2019, title={Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework}, url={http://arxiv.org/abs/1910.04708}, abstractNote={Learning multilingual representations of text has proven a successful method for many cross-lingual transfer learning tasks. There are two main paradigms for learning such representations: (1) alignment, which maps different independently trained monolingual representations into a shared space, and (2) joint training, which directly learns unified multilingual representations using monolingual and cross-lingual objectives jointly. In this paper, we first conduct direct comparisons of representations learned using both of these methods across diverse cross-lingual tasks. Our empirical results reveal a set of pros and cons for both methods, and show that the relative performance of alignment versus joint training is task-dependent. Stemming from this analysis, we propose a simple and novel framework that combines these two previously mutually-exclusive approaches. Extensive experiments on various tasks demonstrate that our proposed framework alleviates limitations of both approaches, and outperforms existing methods on the MUSE bilingual lexicon induction (BLI) benchmark. We further show that our proposed framework can generalize to contextualized representations and achieves state-of-the-art results on the CoNLL cross-lingual NER benchmark.}, note={arXiv: 1910.04708}, journal={arXiv:1910.04708 [cs]}, author={Wang, Zirui and Xie, Jiateng and Xu, Ruochen and Yang, Yiming and Neubig, Graham and Carbonell, Jaime}, year={2019}, month={Oct} }

@article{AttentionVaswani2017, title={Attention Is All You Need}, url={http://arxiv.org/abs/1706.03762}, abstractNote={The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.}, note={arXiv: 1706.03762}, journal={arXiv:1706.03762 [cs]}, author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia}, year={2017}, month={Dec} }

@article{Sennrich_Haddow_Birch_2016, title={Neural Machine Translation of Rare Words with Subword Units}, url={http://arxiv.org/abs/1508.07909}, abstractNote={Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.}, note={arXiv: 1508.07909}, journal={arXiv:1508.07909 [cs]}, author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra}, year={2016}, month={Jun} }

@inproceedings{Xing_Wang_Liu_Lin, title={Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation}, booktitle={Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, author={Xing, Chao and Wang, Dong and Liu, Chao and Lin, Yiye}, year={2015} }

@article{Devlin_Chang_Lee_Toutanova_2019, title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, url={http://arxiv.org/abs/1810.04805}, abstractNote={We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}, note={arXiv: 1810.04805}, journal={arXiv:1810.04805 [cs]}, author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina}, year={2019}, month={May} }

 
@inproceedings{paszke2017automatic,
  title={Automatic Differentiation in {PyTorch}},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  booktitle={NeurIPS Autodiff Workshop},
  year={2017}
}

@article{GageBPE1994, title={A New Algorithm for Data Compression}, volume={12}, ISSN={0898-9788}, number={2}, journal={C Users J.}, author={Gage, Philip}, year={1994}, month={Feb}, pages={23–38} }

@inproceedings{Tiedemann2012, place={Avignon, France}, title={Character-Based Pivot Translation for Under-Resourced Languages and Domains}, url={https://www.aclweb.org/anthology/E12-1015}, booktitle={Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics}, publisher={Association for Computational Linguistics}, author={Tiedemann, Jörg}, year={2012}, month={Apr}, pages={141–151} }

@inproceedings{FarhanKhodra2017, title={Sentiment-specific word embedding for Indonesian sentiment analysis}, ISSN={null}, DOI={10.1109/ICAICTA.2017.8090964}, abstractNote={Word embedding has been proven to improve performance of sentiment classification systems. A new proposed word embedding called sentiment-specific word embedding exploits sentiment information, in order to reach better performance of sentiment analysis systems using word embedding. In this paper, we build Indonesian sentiment-specific word embedding and apply it for sentiment analysis. We compare performance of sentiment classification based on sentiment-specific word embedding with other popular feature representations such as bag of words, Term Frequency-Inverse Document Frequency (TF-IDF), and also generic word embedding. Although our sentiment-specific word embedding models achieve a higher score than the Word2Vec embedding, the lexical representations, bag of words and TF-IDF still gained better performance.}, booktitle={2017 International Conference on Advanced Informatics, Concepts, Theory, and Applications (ICAICTA)}, author={Farhan, Ahmad Naufal and Khodra, Masayu Leylia}, year={2017}, month={Aug}, pages={1–5} }

@article{LamplePhrase2018, title={Phrase-Based \& Neural Unsupervised Machine Translation}, url={http://arxiv.org/abs/1804.07755}, abstractNote={Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of parallel sentences, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage a careful initialization of the parameters, the denoising effect of language models and automatic generation of parallel data by iterative back-translation. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT’14 English-French and WMT’16 German-English benchmarks, our models respectively obtain 28.1 and 25.2 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points. On low-resource languages like English-Urdu and English-Romanian, our methods achieve even better results than semi-supervised and supervised approaches leveraging the paucity of available bitexts. Our code for NMT and PBSMT is publicly available.}, note={arXiv: 1804.07755}, journal={arXiv:1804.07755 [cs]}, author={Lample, Guillaume and Ott, Myle and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc’Aurelio}, year={2018}, month={Aug} }

@inproceedings{HowardRuder2018, place={Melbourne, Australia}, title={Universal Language Model Fine-tuning for Text Classification}, url={https://www.aclweb.org/anthology/P18-1031}, DOI={10.18653/v1/P18-1031}, abstractNote={Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.}, booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, publisher={Association for Computational Linguistics}, author={Howard, Jeremy and Ruder, Sebastian}, year={2018}, month={Jul}, pages={328–339} }

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  url={https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf},
  year={2018}
}

@article{Taylor_1953, title={“Cloze Procedure”: A New Tool for Measuring Readability}, volume={30}, ISSN={0022-5533}, DOI={10.1177/107769905303000401}, abstractNote={Here is the first comprehensive statement of a research method and its theory which were introduced briefly during a workshop at the 1953 AEJ convention. Included are findings from three pilot studies and two experiments in which “cloze procedure” results are compared with those of two readability formulas.}, number={4}, journal={Journalism Quarterly}, author={Taylor, Wilson L.}, year={1953}, month={Sep}, pages={415–433} }

@book{mitchell_machine_1997,
	address = {New York},
	series = {{McGraw}-{Hill} series in computer science},
	title = {Machine {Learning}},
	isbn = {9780070428072},
	publisher = {McGraw-Hill},
	author = {Mitchell, Tom M.},
	year = {1997},
	keywords = {Machine learning, Computer algorithms}
}

@article{lexicon_hatespeech_2015,
author = {Njagi, Dennis and Zuping, Z. and Hanyurwimfura, Damien and Long, Jun},
year = {2015},
month = {04},
pages = {215-230},
title = {A Lexicon-based Approach for Hate Speech Detection},
volume = {10},
journal = {International Journal of Multimedia and Ubiquitous Engineering},
doi = {10.14257/ijmue.2015.10.4.21}
}

@inproceedings{Ibrohim_Budi_2019, place={Florence, Italy}, title={Multi-label Hate Speech and Abusive Language Detection in Indonesian Twitter}, url={https://www.aclweb.org/anthology/W19-3506}, DOI={10.18653/v1/W19-3506}, abstractNote={Hate speech and abusive language spreading on social media need to be detected automatically to avoid conflict between citizen. Moreover, hate speech has a target, category, and level that also needs to be detected to help the authority in prioritizing which hate speech must be addressed immediately. This research discusses multi-label text classification for abusive language and hate speech detection including detecting the target, category, and level of hate speech in Indonesian Twitter using machine learning approach with Support Vector Machine (SVM), Naive Bayes (NB), and Random Forest Decision Tree (RFDT) classifier and Binary Relevance (BR), Label Power-set (LP), and Classifier Chains (CC) as the data transformation method. We used several kinds of feature extractions which are term frequency, orthography, and lexicon features. Our experiment results show that in general RFDT classifier using LP as the transformation method gives the best accuracy with fast computational time.}, booktitle={Proceedings of the Third Workshop on Abusive Language Online}, publisher={Association for Computational Linguistics}, author={Ibrohim, Muhammad Okky and Budi, Indra}, year={2019}, month={Aug}, pages={46–57} }

@misc{Diksusi_Bareskrim,
  author       = {Bayu Hernanto and Jeihan},
  title        = {},
  howpublished = {Personal communication},
  year         = {2018}
}

@article{Conneau_XLMR, title={Unsupervised Cross-lingual Representation Learning at Scale}, url={http://arxiv.org/abs/1911.02116}, abstractNote={This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.}, note={arXiv: 1911.02116}, journal={arXiv:1911.02116 [cs]}, author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin}, year={2020}, month={Apr} }

@misc{tensorflow2015,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@inproceedings{Wei_Shi_Yang_2007, place={Berlin, Heidelberg}, series={Lecture Notes in Computer Science}, title={Feature Reinforcement Approach to Poly-lingual Text Categorization}, ISBN={9783540770947}, DOI={10.1007/978-3-540-77094-7_17}, abstractNote={With the rapid emergence and proliferation of Internet and the trend of globalization, a tremendous amount of textual documents written in different languages are electronically accessible online. Poly-lingual text categorization (PLTC) refers to the automatic learning of a text categorization model(s) from a set of preclassified training documents written in different languages and the subsequent assignment of unclassified poly-lingual documents to predefined categories on the basis of the induced text categorization model(s). Although PLTC can be approached as multiple independent monolingual text categorization problems, this naïve approach employs only the training documents of the same language to construct a monolingual classifier and fails to utilize the opportunity offered by poly-lingual training documents. In this study, we propose a feature reinforcement approach to PLTC that takes into account the training documents of all languages when constructing a monolingual classifier for a specific language. Using the independent monolingual text categorization (MnTC) technique as performance benchmarks, our empirical evaluation results show that the proposed PLTC technique achieves higher classification accuracy than the benchmark technique does in both English and Chinese corpora.}, booktitle={Asian Digital Libraries. Looking Back 10 Years and Forging New Frontiers}, publisher={Springer}, author={Wei, Chih-Ping and Shi, Huihua and Yang, Christopher C.}, editor={Goh, Dion Hoe-Lian and Cao, Tru Hoang and Sølvberg, Ingeborg Torvik and Rasmussen, Edie}, year={2007}, pages={99–108}, collection={Lecture Notes in Computer Science} }

@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@article{Wolf_Debut_Sanh_Chaumond_Delangue_Moi_Cistac_Rault_Louf_Funtowicz, title={HuggingFace’s Transformers: State-of-the-art Natural Language Processing}, url={http://arxiv.org/abs/1910.03771}, abstractNote={Recent advances in modern Natural Language Processing (NLP) research have been dominated by the combination of Transfer Learning methods with large-scale language models, in particular based on the Transformer architecture. With them came a paradigm shift in NLP with the starting point for training a model on a downstream task moving from a blank specific model to a general-purpose pretrained architecture. Still, creating these general-purpose models remains an expensive and time-consuming process restricting the use of these methods to a small sub-set of the wider NLP community. In this paper, we present HuggingFace’s Transformers library, a library for state-of-the-art NLP, making these developments available to the community by gathering state-of-the-art general-purpose pretrained models under a unified API together with an ecosystem of libraries, examples, tutorials and scripts targeting many downstream NLP tasks. HuggingFace’s Transformers library features carefully crafted model implementations and high-performance pretrained weights for two main deep learning frameworks, PyTorch and TensorFlow, while supporting all the necessary tools to analyze, evaluate and use these models in downstream tasks such as text/token classification, questions answering and language generation among others. The library has gained significant organic traction and adoption among both the researcher and practitioner communities. We are committed at HuggingFace to pursue the efforts to develop this toolkit with the ambition of creating the standard library for building NLP systems. HuggingFace’s Transformers library is available at url{https://github.com/huggingface/transformers}.}, note={arXiv: 1910.03771}, journal={arXiv:1910.03771 [cs]}, author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and et al.}, year={2020}, month={Feb} }


@article{Pires_Schlinger_Garrette_2019, title={How multilingual is Multilingual BERT?}, url={http://arxiv.org/abs/1906.01502}, abstractNote={In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.}, note={arXiv: 1906.01502}, journal={arXiv:1906.01502 [cs]}, author={Pires, Telmo and Schlinger, Eva and Garrette, Dan}, year={2019}, month={Jun} }

@article{Liu_Ott_Goyal_Du_Joshi_Chen_Levy_Lewis_Zettlemoyer_Stoyanov_2019, title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, url={http://arxiv.org/abs/1907.11692}, abstractNote={Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.}, note={arXiv: 1907.11692}, journal={arXiv:1907.11692 [cs]}, author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin}, year={2019}, month={Jul} }

@article{Peters_Ruder_Smith_2019, title={To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks}, url={http://arxiv.org/abs/1903.05987}, abstractNote={While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.}, note={arXiv: 1903.05987}, journal={arXiv:1903.05987 [cs]}, author={Peters, Matthew E. and Ruder, Sebastian and Smith, Noah A.}, year={2019}, month={Jun} }